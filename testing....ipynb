{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a735a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generic imports\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "import itertools\n",
    "import os.path\n",
    "\n",
    "#track models\n",
    "import wandb\n",
    "\n",
    "#import constants\n",
    "from epi_to_express.constants import (\n",
    "    CHROM_LEN, \n",
    "    CHROMOSOMES, \n",
    "    SAMPLES,\n",
    "    SAMPLE_IDS,\n",
    "    CHROMOSOME_DATA,\n",
    "    SRC_PATH,\n",
    "    ASSAYS,\n",
    "    PROJECT_PATH)\n",
    "\n",
    "#model imports\n",
    "#data loading imports\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from chromoformer.chromoformer.data import Roadmap3D\n",
    "from chromoformer.chromoformer.net import Chromoformer\n",
    "#from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "from chromoformer.chromoformer.util import seed_everything\n",
    "\n",
    "#pass inputs\n",
    "# argv\n",
    "#def get_args():\n",
    "#    parser = argparse.ArgumentParser(description=\"train\")\n",
    "#    parser.add_argument('-c', '--CELL', default='', type=str, help='Cell to train in')\n",
    "#    parser.add_argument('-m', '--MARK', default='', type=str, help='Mark to train on')\n",
    "#    args = parser.parse_args()\n",
    "#    return args\n",
    "\n",
    "#args=get_args()\n",
    "\n",
    "CELL='E003'#args.CELL\n",
    "#leading and trailing whitespace\n",
    "CELL=CELL.strip()\n",
    "#assert it's a valid choice\n",
    "assert CELL in SAMPLE_IDS, f\"{CELL} not valid. Must choose valid cell: {SAMPLE_IDS}\"\n",
    "\n",
    "MARK='h3k27ac'#'h3k4me1'#args.MARK.lower()\n",
    "MARK=MARK.strip()\n",
    "#assert it's a valid choice\n",
    "assert MARK in ASSAYS, f\"{MARK} not valid. Must choose valid assay: {ASSAYS}\"\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(CELL)\n",
    "print(MARK)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "seed_everything(101)\n",
    "\n",
    "SAVE_PATH = pathlib.Path(\"./model_results\")\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MOD_SAVE_PATH = pathlib.Path(\"./model_results/models\")\n",
    "MOD_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. --- SETUP PARAMETERS ------------------------------------------------\n",
    "\n",
    "#what will be used to predict expression\n",
    "features = ['h3k4me1', 'h3k4me3', 'h3k9me3', 'h3k27me3', 'h3k36me3', 'h3k27ac', 'h3k9ac']#[MARK]\n",
    "#what cell will we predict in\n",
    "cell = CELL\n",
    "# 1 Mb of the assay will be considered for the prediction of gene expression\n",
    "window_size = 40_000\n",
    "#number of k-fold cross validation\n",
    "k_fold = 4\n",
    "#seed\n",
    "seed = 123\n",
    "#regression problem\n",
    "y_type = 'log2RPKM'\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Model specifics - similar to https://www.nature.com/articles/s42256-022-00570-9\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "\n",
    "lr = 3e-5\n",
    "gamma = 0.87\n",
    "i_max = 8\n",
    "embed_n_layers = 1\n",
    "embed_n_heads = 2\n",
    "embed_d_model = 128\n",
    "embed_d_ff = 128\n",
    "pw_int_n_layers = 2\n",
    "pw_int_n_heads = 2\n",
    "pw_int_d_model = 128\n",
    "pw_int_d_ff = 256\n",
    "reg_n_layers = 6\n",
    "reg_n_heads = 8\n",
    "reg_d_model = 256\n",
    "reg_d_ff = 256\n",
    "head_n_feats = 128\n",
    "\n",
    "# 2. --- Dataset parameters -------------------------------\n",
    "train_dir = PROJECT_PATH/'chromoformer'/'preprocessing'\n",
    "train_meta = train_dir / 'train.csv'\n",
    "meta = pd.read_csv(train_meta) \\\n",
    "    .sample(frac=1, random_state=seed) \\\n",
    "    .reset_index(drop=True) # load and shuffle.\n",
    "\n",
    "#filter metadat to cell type of interest\n",
    "meta = meta[meta.eid == CELL]\n",
    "\n",
    "# Split genes into two sets (train/val).\n",
    "genes = set(meta.gene_id.unique())\n",
    "n_genes = len(genes)\n",
    "print('Target genes:', len(genes))\n",
    "\n",
    "#get data for folds separated\n",
    "qs = [\n",
    "    meta[meta.split == 1].gene_id.tolist(),\n",
    "    meta[meta.split == 2].gene_id.tolist(),\n",
    "    meta[meta.split == 3].gene_id.tolist(),\n",
    "    meta[meta.split == 4].gene_id.tolist(),\n",
    "]\n",
    "\n",
    "# 3. --- Train models -------------------------------\n",
    "# loop through each fold\n",
    "for ind,fold in enumerate([x+1 for x in range(k_fold)]):\n",
    "    if not os.path.exists(str(f\"{MOD_SAVE_PATH}/chromoformer_{cell}_{'-'.join(features)}_kfold{fold}\")):\n",
    "        print(f\"K-fold Cross-Validation - blind test: {ind}\")\n",
    "        #get fold specific data ----\n",
    "        train_genes = qs[(fold + 0) % 4] + qs[(fold + 1) % 4] + qs[(fold + 2) % 4]\n",
    "        val_genes = qs[(fold + 3) % 4]\n",
    "\n",
    "        #split val_genes in two to get validation and test set\n",
    "        # train/val split by chrom so do the same for val test\n",
    "        val_test_genes = val_genes\n",
    "        val_test_chrom = list(set(meta[meta.gene_id.isin(val_test_genes)]['chrom']))\n",
    "        val_chrom = val_test_chrom[0:len(val_test_chrom)//2]\n",
    "        test_chrom = val_test_chrom[len(val_test_chrom)//2:len(val_test_chrom)]\n",
    "        val_genes = meta[meta.gene_id.isin(val_test_genes) & meta.chrom.isin(val_chrom)]['gene_id'].tolist()\n",
    "        test_genes = meta[meta.gene_id.isin(val_test_genes) & meta.chrom.isin(test_chrom)]['gene_id'].tolist()\n",
    "        #----\n",
    "        # 2. --- Dataset parameters -------------------------------\n",
    "        train_dataset = Roadmap3D(cell, train_genes, i_max, window_size, window_size,\n",
    "                                  marks=features,train_dir=train_dir,train_meta=train_meta)\n",
    "        val_dataset = Roadmap3D(cell, val_genes, i_max, window_size, window_size,\n",
    "                                marks=features,train_dir=train_dir,train_meta=train_meta)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                   num_workers=8, shuffle=True, drop_last=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
    "                                                 num_workers=8)\n",
    "        print(aa)\n",
    "\n",
    "        model = Chromoformer(\n",
    "            n_feats=len(features), embed_n_layers=embed_n_layers, #1 feature input\n",
    "            embed_n_heads = embed_n_heads, embed_d_model=embed_d_model, \n",
    "            embed_d_ff=embed_d_ff, pw_int_n_layers=pw_int_n_layers, \n",
    "            pw_int_n_heads=pw_int_n_heads, pw_int_d_model=pw_int_d_model, \n",
    "            pw_int_d_ff=pw_int_d_ff,reg_n_layers=reg_n_layers, \n",
    "            reg_n_heads=reg_n_heads, reg_d_model=reg_d_model, \n",
    "            reg_d_ff=reg_d_ff, head_n_feats=head_n_feats\n",
    "        )\n",
    "        model.cuda()\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr))\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        #----\n",
    "        #save to wandb if ind = 1\n",
    "        if fold==1:\n",
    "            readable_features = '-'.join(features)\n",
    "            wandb.init(\n",
    "                name=f'chromoformer_{cell}_{readable_features}_{fold}',\n",
    "                entity=\"al-murphy\",\n",
    "                project=\"Epi_to_Express\",\n",
    "            )\n",
    "        \n",
    "        for epoch in range(0, n_epochs):\n",
    "            print('epoch',epoch)\n",
    "            # Prepare train.\n",
    "            bar = tqdm(enumerate(train_loader, 1), total=len(train_loader))\n",
    "            running_loss = 0.0\n",
    "            train_out, train_label = [], []\n",
    "\n",
    "            # Train.\n",
    "            model.train()\n",
    "            for batch, d in bar:\n",
    "                for k, v in d.items():\n",
    "                    d[k] = v.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = model(\n",
    "                    d['x_p_2000'], d['pad_mask_p_2000'], d['x_pcre_2000'], \n",
    "                    d['pad_mask_pcre_2000'], d['interaction_mask_2000'],\n",
    "                    d['x_p_500'], d['pad_mask_p_500'], d['x_pcre_500'], \n",
    "                    d['pad_mask_pcre_500'], d['interaction_mask_2000'],\n",
    "                    d['x_p_100'], d['pad_mask_p_100'], d['x_pcre_100'], \n",
    "                    d['pad_mask_pcre_100'], d['interaction_mask_2000'],\n",
    "                    d['interaction_freq'],\n",
    "                )\n",
    "                y = d['log2RPKM'].float().unsqueeze(1)\n",
    "                loss = criterion(out,y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss = loss.detach().cpu().item()\n",
    "                running_loss += loss\n",
    "\n",
    "                train_out.append(out.detach().cpu())\n",
    "                train_label.append(d['log2RPKM'].unsqueeze(1).cpu())\n",
    "                \n",
    "                #save training error at end of epoch\n",
    "                if batch == len(train_loader):\n",
    "                    print('batch == len(train_loader):')\n",
    "                    print('batch',batch)\n",
    "                    batch_loss = running_loss / len(train_loader)\n",
    "\n",
    "                    train_out, train_label = map(torch.cat, (train_out, train_label))\n",
    "                    #train_score = train_out.softmax(axis=1)[:, 1]\n",
    "                    #train_pred = train_out.argmax(axis=1)\n",
    "                    \n",
    "                    batch_mse = metrics.mean_squared_error(train_label, train_out)\n",
    "                    print(\"train_label.shape\",train_label.shape)\n",
    "                    print(\"train_out.shape\",train_out.shape)\n",
    "                    from torchmetrics.functional import pearson_corrcoef\n",
    "                    print(pearson_corrcoef(train_label, train_out))\n",
    "                    batch_corr = stats.pearsonr(train_label, train_out)\n",
    "                    bar.set_description(f'E{epoch} {batch_loss:.4f}, lr={get_lr(optimizer)}, mse={batch_mse}, corr={batch_corr.statistic}')\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    train_out, train_label = [], []\n",
    "                    if fold==1:\n",
    "                        wandb.log({\n",
    "                            'loss': batch_loss,\n",
    "                            'mse': batch_mse,\n",
    "                            'correlation': batch_corr.statistic,\n",
    "                            'lr': get_lr(optimizer),\n",
    "                            'epoch': epoch,\n",
    "                        })\n",
    "\n",
    "            # Prepare validation.\n",
    "            bar = tqdm(enumerate(val_loader, 1), total=len(val_loader))\n",
    "            val_out, val_label = [], []\n",
    "\n",
    "            # Validation.\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch, d in bar:\n",
    "                    for k, v in d.items():\n",
    "                        d[k] = v.cuda()\n",
    "\n",
    "                    out = model(\n",
    "                        d['x_p_2000'], d['pad_mask_p_2000'], d['x_pcre_2000'], \n",
    "                        d['pad_mask_pcre_2000'], d['interaction_mask_2000'],\n",
    "                        d['x_p_500'], d['pad_mask_p_500'], d['x_pcre_500'], \n",
    "                        d['pad_mask_pcre_500'], d['interaction_mask_2000'],\n",
    "                        d['x_p_100'], d['pad_mask_p_100'], d['x_pcre_100'], \n",
    "                        d['pad_mask_pcre_100'], d['interaction_mask_2000'],\n",
    "                        d['interaction_freq'],\n",
    "                    )\n",
    "                    val_out.append(out.cpu())\n",
    "\n",
    "                    val_label.append(d['log2RPKM'].unsqueeze(1).cpu())\n",
    "\n",
    "            val_out = torch.cat(val_out)\n",
    "            val_label = torch.cat(val_label)\n",
    "\n",
    "            val_loss = criterion(val_out, val_label)\n",
    "\n",
    "            # Metrics.\n",
    "            val_label = val_label.numpy()\n",
    "            #val_score = val_out.softmax(axis=1)[:, 1].numpy()\n",
    "            #val_pred = val_out.argmax(axis=1).numpy()\n",
    "            \n",
    "            val_mse = metrics.mean_squared_error(val_label, val_out)\n",
    "            val_corr = stats.pearsonr(val_label, val_out)\n",
    "\n",
    "            print(f'Validation loss={val_loss:.4f}, mse={val_mse}, corr={val_corr.statistic}')\n",
    "            if fold==1:\n",
    "                wandb.log({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_mse': val_mse,\n",
    "                    'val_correlation': val_corr.statistic,\n",
    "                    'epoch': epoch,\n",
    "                })\n",
    "\n",
    "            #save result after 10 epochs - same as chromoformer publication\n",
    "            if epoch == n_epochs-1:\n",
    "                print(\"epoch == n_epochs\")\n",
    "                print('epoch',epoch)\n",
    "                ckpt = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'last_val_loss': val_loss,\n",
    "                    'last_val_mse': val_mse,\n",
    "                    'last_val_corr': val_corr.statistic,\n",
    "                    'val_act': val_label,\n",
    "                    'val_pred': val_out,\n",
    "                }\n",
    "                torch.save(ckpt, f\"{MOD_SAVE_PATH}/chromoformer_{cell}_{'-'.join(features)}_kfold{fold}\")\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c960d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881fb6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item['log2RPKM'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451c5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromoformer",
   "language": "python",
   "name": "chromoformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
